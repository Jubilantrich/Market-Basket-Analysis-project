{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/03 04:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create an app name\n",
    "spark = SparkSession.builder.appName(\"insta-cart-project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#(Task 1 step 1) Read the orders data as a dataframe in pySpark \n",
    "orders =spark.read.csv(\"insta-cart/insta-cart/orders.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 2539329|      1|   prior|           1|        2|                8|                  NULL|\n",
      "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
      "| 3367565|      1|   prior|           6|        2|                7|                  19.0|\n",
      "|  550135|      1|   prior|           7|        1|                9|                  20.0|\n",
      "| 3108588|      1|   prior|           8|        1|               14|                  14.0|\n",
      "| 2295261|      1|   prior|           9|        1|               16|                   0.0|\n",
      "| 2550362|      1|   prior|          10|        4|                8|                  30.0|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(Task 1 step 2) Display the data up to 10 rows\n",
    "orders.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Task 2 step 1) replacing NULL values with 99 o column days_since_prior_order\n",
    "Task2_step1_result = orders.na.fill(value=999, subset=[\"days_since_prior_order\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 2539329|      1|   prior|           1|        2|                8|                 999.0|\n",
      "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
      "| 3367565|      1|   prior|           6|        2|                7|                  19.0|\n",
      "|  550135|      1|   prior|           7|        1|                9|                  20.0|\n",
      "| 3108588|      1|   prior|           8|        1|               14|                  14.0|\n",
      "| 2295261|      1|   prior|           9|        1|               16|                   0.0|\n",
      "| 2550362|      1|   prior|          10|        4|                8|                  30.0|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (Task 2 step 2) Display the data up to 10 rows\n",
    "Task2_step1_result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering your PySpark DataFrame as a temporary SQL table inside your Spark session\n",
    "orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|total_orders|day_of_week|\n",
      "+------------+-----------+\n",
      "|      600905|     Sunday|\n",
      "|      587478|     Monday|\n",
      "|      467260|    Tuesday|\n",
      "|      453368|     Friday|\n",
      "|      448761|   Saturday|\n",
      "|      436972|  Wednesday|\n",
      "|      426339|   Thursday|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the result that contains the total orders placed on each day of the week (Monday to Sunday) \n",
    "Task3_results = spark.sql(\"\"\"\n",
    "SELECT COUNT(order_id) AS total_orders, (\n",
    "CASE\n",
    "    WHEN order_dow = '0' THEN 'Sunday'\n",
    "    WHEN order_dow = '1' THEN 'Monday'\n",
    "    WHEN order_dow = '2' THEN 'Tuesday'\n",
    "    WHEN order_dow = '3' THEN 'Wednesday'\n",
    "    WHEN order_dow = '4' THEN 'Thursday'\n",
    "    WHEN order_dow = '5' THEN 'Friday'\n",
    "    WHEN order_dow = '6' THEN 'Saturday'\n",
    "END\n",
    ") AS day_of_week\n",
    "FROM orders\n",
    "GROUP BY order_dow\n",
    "ORDER BY total_orders DESC\n",
    "                                   \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 - Select the number of order IDs as Total_Orders and the hour at which the order was placed using spark sql\n",
    "Task4_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(order_id) AS total_orders, \n",
    "    order_hour_of_day AS hour \n",
    "FROM Orders\n",
    "GROUP BY order_hour_of_day\n",
    "ORDER BY order_hour_of_day\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|total_orders|hour|\n",
      "+------------+----+\n",
      "|       22758|   0|\n",
      "|       12398|   1|\n",
      "|        7539|   2|\n",
      "|        5474|   3|\n",
      "|        5527|   4|\n",
      "|        9569|   5|\n",
      "|       30529|   6|\n",
      "|       91868|   7|\n",
      "|      178201|   8|\n",
      "|      257812|   9|\n",
      "|      288418|  10|\n",
      "|      284728|  11|\n",
      "|      272841|  12|\n",
      "|      277999|  13|\n",
      "|      283042|  14|\n",
      "|      283639|  15|\n",
      "|      272553|  16|\n",
      "|      228795|  17|\n",
      "|      182912|  18|\n",
      "|      140569|  19|\n",
      "+------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Task4_result.show() #Display the result that contains total orders and the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## task 5 step 1 Read the order_products__prior.csv and products.csv data\n",
    "products = spark.read.csv(\"insta-cart/insta-cart/products.csv\", header = True, inferSchema = True)\n",
    "order_product =spark.read.csv(\"insta-cart/insta-cart/order_products__prior.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the tables for a particular spark session \n",
    "products.createOrReplaceTempView(\"products\")\n",
    "order_product.createOrReplaceTempView(\"order_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "task5_result=spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(opp.order_id) AS Order_Count,\n",
    "    p.product_name AS Popular_Product_Name\n",
    "FROM order_product opp\n",
    "JOIN products p\n",
    "ON opp.product_id = p.product_id\n",
    "GROUP BY p.product_name\n",
    "ORDER BY order_count DESC\n",
    "LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|Order_Count|Popular_Product_Name|\n",
      "+-----------+--------------------+\n",
      "|     472565|              Banana|\n",
      "|     379450|Bag of Organic Ba...|\n",
      "|     264683|Organic Strawberries|\n",
      "|     241921|Organic Baby Spinach|\n",
      "|     213584|Organic Hass Avocado|\n",
      "|     176815|     Organic Avocado|\n",
      "|     152657|         Large Lemon|\n",
      "|     142951|        Strawberries|\n",
      "|     140627|               Limes|\n",
      "|     137905|  Organic Whole Milk|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the result\n",
    "task5_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 6 & 7 Read the data from the departements.csv file \n",
    "departments = spark.read.csv(\"insta-cart/insta-cart/departments.csv\", header = True, inferSchema = True)\n",
    "# Store the tables for a particular spark session \n",
    "departments.createOrReplaceTempView(\"departments\")\n",
    "task6n7_result=spark.sql(\"\"\"\n",
    "SELECT \n",
    "        department_id AS Department_ID, \n",
    "        COUNT(product_id) AS Max_Product\n",
    "    FROM products\n",
    "    GROUP BY department_id\n",
    "    ORDER BY Max_Product DESC\n",
    "    LIMIT 1\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|Department_ID|Max_Product|\n",
      "+-------------+-----------+\n",
      "|           11|       6563|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the data stored \n",
    "task6n7_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
